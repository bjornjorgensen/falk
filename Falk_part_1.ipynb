{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54ca3a6-ca3e-46b7-a41d-4634c656511a",
   "metadata": {},
   "source": [
    "Links\n",
    "https://op.europa.eu/en/web/eu-vocabularies/e-procurement/tedschemas\n",
    "\n",
    "https://ec.europa.eu/docsroom/documents/27821/attachments/1/translations/en/renditions/native page 22\n",
    "\n",
    "#link to files https://1drv.ms/u/s!Ag_DGaic1jWMlY1HKM2GKxf_o76L3w?e=CptwgP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74ff61-8561-46f3-9b4a-cedb9c39be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import pandas as ps\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "#import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, concat_ws, lit, col, trim, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"]=\"1\"\n",
    "\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster('local[*]')\n",
    "    conf \\\n",
    "      .set('spark.driver.memory', '64g')\\\n",
    "      .set(\"fs.s3a.access.key\", \"minio\") \\\n",
    "      .set(\"fs.s3a.secret.key\", \"minio123\") \\\n",
    "      .set(\"fs.s3a.endpoint\", \"http://192.168.1.127:9000\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "      .set(\"spark.sql.repl.eagerEval.enabled\", \"True\") \\\n",
    "      .set(\"spark.sql.adaptive.enabled\", \"True\") \\\n",
    "      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "      .set(\"spark.sql.repl.eagerEval.maxNumRows\", \"10000\") \\\n",
    "      .set(\"spark.setLogLevel\", \"error\")\n",
    "    \n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()\n",
    "\n",
    "spark = get_spark_session(\"Falk\", SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ce9b8-746b-42bd-9cc8-f72065d0c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_json(dir_path):\n",
    "    path = dir_path \n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('.xml'):\n",
    "            continue\n",
    "\n",
    "        fullname = os.path.join(path, filename)\n",
    "\n",
    "        with open(fullname, 'r') as f:\n",
    "            xmlString = f.read()\n",
    "            \n",
    "        \n",
    "        jsonString = xmltodict.parse(xmlString, process_namespaces=True, attr_prefix='', cdata_key=\"text\")\n",
    "        #jsonString = json.dumps(xmltodict.parse(xmlString, process_namespaces=True, attr_prefix='', cdata_key=\"text\"),ensure_ascii=False)\n",
    "    #jsonString = (xmltodict.parse(xmlString, process_namespaces=True))\n",
    "    #data = json.load(jsonString)\n",
    "    #jsondata = json.load(jsonString, object_hook=lambda d: {int(k): [int(i) for i in v] if isinstance(v, list) else v for k, v in d.items()})\n",
    "    #jsondata2 = json.dumps(jsondata)\n",
    "    #test.import_bulk(data)\n",
    "    #test1 = json.load(jsonString, parse_float=float)\n",
    "    #test.insert(jsonString)\n",
    "    #print(jsonString)\n",
    "\n",
    "        #with open(fullname[:-4] + \".json\", 'w') as f:\n",
    "         #   f.write(jsonString)\n",
    "            \n",
    "                                     \n",
    "        with open(fullname[:-4] + \".json\", 'w', encoding ='utf8') as json_file:\n",
    "                                     json.dump(jsonString, json_file, ensure_ascii = False)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7693d7-7536-4bf0-8eeb-8267d288ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode(o):\n",
    "    # Cassting sting to int or float\n",
    "    \n",
    "    \n",
    "    if isinstance(o, str):\n",
    "        try:\n",
    "            return int(o)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(o)\n",
    "            except ValueError:\n",
    "                return o\n",
    "\n",
    "    elif isinstance(o, dict):\n",
    "        return {k: _decode(v) for k, v in o.items()}\n",
    "    elif isinstance(o, list):\n",
    "        return [_decode(v) for v in o]\n",
    "    else:\n",
    "        return o       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c6e84-b180-4530-b687-cff89ba20743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_iter(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        for item in obj.values():\n",
    "            if \"P\" in obj and isinstance(obj[\"P\"], list):\n",
    "                obj[\"P\"] = \" \".join([str(e) for e in obj[\"P\"]])\n",
    "            else:\n",
    "                yield from recursive_iter(item)\n",
    "    elif any(isinstance(obj, t) for t in (list, tuple)):\n",
    "        for item in obj:\n",
    "            yield from recursive_iter(item)\n",
    "    else:\n",
    "        yield obj\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bf478-296d-40c7-a26a-9e331c0bc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_strip_list2(dir_path):\n",
    "    path = dir_path \n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "\n",
    "        fullname = os.path.join(path, filename)\n",
    "        with open(fullname) as json_file:\n",
    "            jsonstr = json.load(json_file, object_hook= _decode)\n",
    "    \n",
    "        #with open(fullname, 'r') as f:\n",
    "         #   jsonstr = f.read()\n",
    "\n",
    "        #json_sting = json.loads(jsonstr)\n",
    "        for item in recursive_iter(jsonstr):\n",
    "            with open(fullname[:-4] + \"_stript_list.json\", 'w', encoding ='utf8') as f:\n",
    "                json.dump(jsonstr, f, ensure_ascii = False)\n",
    "            #json_str2 = json.dump(jsonstr)\n",
    "        \n",
    "        \n",
    "        #with open(fullname[:-4] + \"_stript_list.json\", 'w') as f:\n",
    "         #   json.dump(json_str2, f)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #with open(fullname[:-4] + \"_stript_list.json\", 'w') as f:\n",
    "         #   f.write(json_str2)   \n",
    "         \n",
    "         \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f287ca-9f02-4102-8a04-7595d0124be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from copy import copy\n",
    "\n",
    "# We take a dataframe and return a new one with required changes\n",
    "def cleanDataFrame(df: DataFrame) -> DataFrame:\n",
    "    # Returns a new sanitized field name (this function can be anything really)\n",
    "    def sanitizeFieldName(s: str) -> str:\n",
    "        return s.replace(\"-\", \"_\").replace(\"&\", \"_\").replace(\"\\\"\", \"_\")\\\n",
    "            .replace(\"[\", \"_\").replace(\"]\", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "    # We call this on all fields to create a copy and to perform any changes we might\n",
    "    # want to do to the field.\n",
    "    def sanitizeField(field: StructField) -> StructField:\n",
    "        field = copy(field)\n",
    "        field.name = sanitizeFieldName(field.name)\n",
    "        # We recursively call cleanSchema on all types\n",
    "        field.dataType = cleanSchema(field.dataType)\n",
    "        return field\n",
    "\n",
    "    def cleanSchema(dataType: [DataType]) -> [DateType]:\n",
    "        dataType = copy(dataType)\n",
    "        # If the type is a StructType we need to recurse otherwise we can return since\n",
    "        # we've reached the leaf node\n",
    "        if isinstance(dataType, StructType):\n",
    "            # We call our sanitizer for all top level fields\n",
    "            dataType.fields = [sanitizeField(f) for f in dataType.fields]\n",
    "        elif isinstance(dataType, ArrayType):\n",
    "            dataType.elementType = cleanSchema(dataType.elementType)\n",
    "        return dataType\n",
    "\n",
    "    # Now since we have the new schema we can create a new DataFrame by using the old Frame's RDD as data and the new schema as the schema for the data\n",
    "    return spark.createDataFrame(df.rdd, cleanSchema(df.schema))     \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641f01a-5835-4c66-bb7c-c3a2ddd16409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "def flatten_test(df, sep=\"_\"):\n",
    "    \"\"\"Returns a flattend dataframe.\n",
    "        .. versionadded:: x.X.X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sep : str\n",
    "            Delimeter for flatted columns. Default `_`\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Don`t use `.` as `sep`\n",
    "        It won`t work on nested dataframes with more than one level.\n",
    "        And you will have to use `columns.name`. \n",
    "        \n",
    "        Flattening Maptypes will have to find every key in the column. \n",
    "        This can be slow.\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "\n",
    "        data_mixed = [\n",
    "            {\n",
    "                \"state\": \"Florida\",\n",
    "                \"shortname\": \"FL\",\n",
    "                \"info\": {\"governor\": \"Rick Scott\"},\n",
    "                \"counties\": [\n",
    "                    {\"name\": \"Dade\", \"population\": 12345},\n",
    "                    {\"name\": \"Broward\", \"population\": 40000},\n",
    "                    {\"name\": \"Palm Beach\", \"population\": 60000},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"state\": \"Ohio\",\n",
    "                \"shortname\": \"OH\",\n",
    "                \"info\": {\"governor\": \"John Kasich\"},\n",
    "                \"counties\": [\n",
    "                    {\"name\": \"Summit\", \"population\": 1234},\n",
    "                    {\"name\": \"Cuyahoga\", \"population\": 1337},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        data_mixed = spark.createDataFrame(data=data_mixed)\n",
    "\n",
    "        data_mixed.printSchema()\n",
    "\n",
    "        root\n",
    "        |-- counties: array (nullable = true)\n",
    "        |    |-- element: map (containsNull = true)\n",
    "        |    |    |-- key: string\n",
    "        |    |    |-- value: string (valueContainsNull = true)\n",
    "        |-- info: map (nullable = true)\n",
    "        |    |-- key: string\n",
    "        |    |-- value: string (valueContainsNull = true)\n",
    "        |-- shortname: string (nullable = true)\n",
    "        |-- state: string (nullable = true)\n",
    "        \n",
    "        \n",
    "        data_mixed_flat = flatten_test(df, sep=\":\")\n",
    "        data_mixed_flat.printSchema()\n",
    "        root\n",
    "        |-- shortname: string (nullable = true)\n",
    "        |-- state: string (nullable = true)\n",
    "        |-- counties:name: string (nullable = true)\n",
    "        |-- counties:population: string (nullable = true)\n",
    "        |-- info:governor: string (nullable = true)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        data = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"Cole Volk\",\n",
    "                \"fitness\": {\"height\": 130, \"weight\": 60},\n",
    "            },\n",
    "            {\"name\": \"Mark Reg\", \"fitness\": {\"height\": 130, \"weight\": 60}},\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"Faye Raker\",\n",
    "                \"fitness\": {\"height\": 130, \"weight\": 60},\n",
    "            },\n",
    "        ]\n",
    "\n",
    "\n",
    "        df = spark.createDataFrame(data=data)\n",
    "\n",
    "        df.printSchema()\n",
    "\n",
    "        root\n",
    "        |-- fitness: map (nullable = true)\n",
    "        |    |-- key: string\n",
    "        |    |-- value: long (valueContainsNull = true)\n",
    "        |-- id: long (nullable = true)\n",
    "        |-- name: string (nullable = true)\n",
    "        \n",
    "        df_flat = flatten_test(df, sep=\":\")\n",
    "\n",
    "        df_flat.printSchema()\n",
    "\n",
    "        root\n",
    "        |-- id: long (nullable = true)\n",
    "        |-- name: string (nullable = true)\n",
    "        |-- fitness:height: long (nullable = true)\n",
    "        |-- fitness:weight: long (nullable = true)\n",
    "        \n",
    "        data_struct = [\n",
    "                ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "                ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "                ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "                ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "                ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "                ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "                ]\n",
    "\n",
    "                \n",
    "        schema = StructType([\n",
    "            StructField('name', StructType([\n",
    "                StructField('firstname', StringType(), True),\n",
    "                StructField('middlename', StringType(), True),\n",
    "                StructField('lastname', StringType(), True)\n",
    "                ])),\n",
    "            StructField('state', StringType(), True),\n",
    "            StructField('gender', StringType(), True)\n",
    "            ])\n",
    "\n",
    "        df_struct = spark.createDataFrame(data = data_struct, schema = schema)\n",
    "\n",
    "        df_struct.printSchema()\n",
    "\n",
    "        root\n",
    "        |-- name: struct (nullable = true)\n",
    "        |    |-- firstname: string (nullable = true)\n",
    "        |    |-- middlename: string (nullable = true)\n",
    "        |    |-- lastname: string (nullable = true)\n",
    "        |-- state: string (nullable = true)\n",
    "        |-- gender: string (nullable = true)\n",
    "        \n",
    "        df_struct_flat = flatten_test(df_struct, sep=\":\")\n",
    "\n",
    "        df_struct_flat.printSchema()\n",
    "\n",
    "        root\n",
    "        |-- state: string (nullable = true)\n",
    "        |-- gender: string (nullable = true)\n",
    "        |-- name:firstname: string (nullable = true)\n",
    "        |-- name:middlename: string (nullable = true)\n",
    "        |-- name:lastname: string (nullable = true)\n",
    "        \"\"\"\n",
    "    # compute Complex Fields (Arrays, Structs and Maptypes) in Schema   \n",
    "    complex_fields = dict([(field.name, field.dataType)\n",
    "                            for field in df.schema.fields\n",
    "                            if type(field.dataType) == ArrayType \n",
    "                            or type(field.dataType) == StructType\n",
    "                            or type(field.dataType) == MapType])\n",
    "    \n",
    "    while len(complex_fields) !=0:\n",
    "        col_name = list(complex_fields.keys())[0]\n",
    "        #print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "        # if StructType then convert all sub element to columns.\n",
    "        # i.e. flatten structs\n",
    "        if (type(complex_fields[col_name]) == StructType):\n",
    "            expanded = [col(col_name + '.' + k).alias(col_name + sep + k) \n",
    "            for k in [n.name for n in complex_fields[col_name]]]\n",
    "            df = df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "        # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "        # i.e. explode Arrays\n",
    "        elif (type(complex_fields[col_name]) == ArrayType):\n",
    "            df = df.withColumn(col_name, explode_outer(col_name))\n",
    "        \n",
    "        # if MapType then convert all sub element to columns.\n",
    "        # i.e. flatten\n",
    "        elif (type(complex_fields[col_name]) == MapType):\n",
    "            keys_df = df.select(explode_outer(map_keys(col(col_name)))).distinct()\n",
    "            keys = list(map(lambda row: row[0], keys_df.collect()))\n",
    "            key_cols = list(map(lambda f: col(col_name).getItem(f)\n",
    "            .alias(str(col_name + sep + f)), keys))\n",
    "            drop_column_list = [col_name]\n",
    "            df = df.select([col_name for col_name in df.columns \n",
    "            if col_name not in drop_column_list] + key_cols)\n",
    "    \n",
    "        # recompute remaining Complex Fields in Schema       \n",
    "        complex_fields = dict([(field.name, field.dataType)\n",
    "                            for field in df.schema.fields\n",
    "                            if type(field.dataType) == ArrayType\n",
    "                            or type(field.dataType) == StructType\n",
    "                            or type(field.dataType) == MapType])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50862a-d86a-449b-b330-b2e581dc71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_norm(dir_path, path_to_save):\n",
    "    path = dir_path \n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('._stript_list.json'):\n",
    "            continue\n",
    "\n",
    "\n",
    "        fullname = os.path.join(path, filename)\n",
    "        with open(fullname) as json_file:\n",
    "            jsonstr = json.load(json_file)\n",
    "    \n",
    "        #with open(fullname, 'r') as f:\n",
    "         #   jsonstr = f.read()\n",
    "\n",
    "        #json_sting = json.loads(jsonstr)\n",
    "        #for item in jsonstr():\n",
    "        df = spark.read.json(fullname)\n",
    "        df = cleanDataFrame(df)\n",
    "        df = flatten(df)\n",
    "        df.write.mode('append').json(path_to_save)\n",
    "        #df3.write.mode('append').json('/home/jovyan/notebooks/falk/F02.json')\n",
    "        #df3.coalesce(1).write.mode('append').json('/home/jovyan/notebooks/falk/F01.json')\n",
    "        \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ddbfa9-af8a-48a2-8e1a-2ae2d53cbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Then, I define the working directory:\n",
    "\n",
    "#https://stackoverflow.com/questions/31346790/unzip-all-zipped-files-in-a-folder-to-that-same-folder-using-python-2-7-5\n",
    "\n",
    "working_directory = '/home/jovyan/notebooks/falk/data/new_files_010721'\n",
    "os.chdir(working_directory)\n",
    "\n",
    "#After that you can use a combination of the os and zipfile to get where you want:\n",
    "#run this 4 times! if you get folders zip files in folders ain't unzipt\n",
    "for file in os.listdir(working_directory):   # get the list of files\n",
    "    if zipfile.is_zipfile(file): # if it is a zipfile, extract it\n",
    "        with zipfile.ZipFile(file) as item: # treat the file as a zip\n",
    "           item.extractall()  # extract it in the working directory        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
